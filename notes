I'm using the Rotten Tomatoes "polarity" dataset (Cornell) and the Film Corpus 2.0 (UCSC).

The corpus scripts had filenames with only lowercase alphanumeric characters and periods, so I made a list of titles from the polarity dataset and compressed the titles in the same way.

I used `comm` to make a list of the common titles (510 left).

I made a mapping of titles to their compressed versions, removing ambiguous titles that weren't in both sets anyway.

I used BeautifulSoup to scrape the relevant data (movie name, reviewer name, review text) from the review html files.

I used these steps to preprocess the scripts:

    from ~/film_aggs/scripts_common:

    $ for i in *; do <$i grep -v '^<' > ../scripts_common_cleaned/$i; done

    from ~/film_aggs:

    $ foo=scripts_common_cleaned; n=1; m=$(($n + 1)); mkdir -p ${foo}_${m}; for i in $(ls ${foo}_${n}); do < ${foo}_${n}/${i} sed 's/\s\+/ /g' > ${foo}_${m}/${i}; done

    $ foo=scripts_common_cleaned; n=2; m=$(($n + 1)); mkdir -p ${foo}_${m}; for i in $(ls ${foo}_${n}); do < ${foo}_${n}/${i} sed 's/\s*$//g' > ${foo}_${m}/${i}; done

    $ foo=scripts_common_cleaned; n=3; m=$(($n + 1)); mkdir -p ${foo}_${m}; for i in $(ls ${foo}_${n}); do < ${foo}_${n}/${i} sed 's/^[^a-zA-Z]*$//g' > ${foo}_${m}/${i}; done

    for i in *; do <$i grep -v '^<' > ../scripts_common_cleaned/$i; done

    scripts_common_cleaned_5 has scripts deleted up until first scene (no title etc.) (manually, with vim macros)
